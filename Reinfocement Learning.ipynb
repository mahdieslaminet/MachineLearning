{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd76de9",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518b7b2",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>\n",
    "یادگیری تقویتی یا Reinforcement Learning نوعی یادگیری ماشین است که با فرایندهای تصمیم‌گیری متوالی سروکار دارد و شامل یک عامل (Agent)، یک محیط (Enivronment) و یک مکانیسم بازخورد برای هدایت اقدامات عامل است. عامل یاد می‌گیرد که اقداماتی را در محیط انجام دهد تا سیگنال پاداش تجمعی را به حداکثر برساند. این سیگانال پاداش جمعی به‌عنوان نیروی محرکه برای یادگیری عمل می‌کند"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d5474",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>\n",
    "نحوه‌ی عملکرد یادگیری تقویتی:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33642da",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>\n",
    "یادگیری تقویتی را می‌توان به‌عنوان یک حلقه متشکل از اجزای زیر در نظر گرفت:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583b45b",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    " \n",
    "+ عامل (Agent): یادگیرنده یا تصمیم‌گیرنده‌ای که براساس مشاهده‌های خود اقداماتی را انجام می‌دهد  \n",
    "      \n",
    "+ محیط (Environment): سیستم یا زمینه‌ی خارجی که عامل در آن عمل می‌کند  \n",
    "      \n",
    "+ حالت (State): پیکربندی یا نمایش فعلی محیط در یک زمان معین  \n",
    "      \n",
    "+ اقدام (Action) : تصمیم یا انتخابی که عامل در پاسخ به یک حالت اتخاذ می‌کند  \n",
    "      \n",
    "+ پاداش (Reward): سیگنال بازخوردی که خوبی یا مطلوبیت عمل عامل را ارزیابی می‌کند  \n",
    "      \n",
    "+ خط‌مشی (Policy): استراتژی یا رویکردی که عامل برای انتخاب اقدامات براساس حالت‌های مشاهده‌شده به کار می‌گیرد  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53881b9",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "\n",
    "یادگیری تقویتی (RL) یک الگوی یادگیری است که در آن یک عامل یاد می‌گیرد که با تعامل با یک محیط تصمیم‌های متوالی بگیرد. عامل براساس اقدامات خود بازخوردی را در قالب پاداش یا جریمه دریافت می‌کند. هدف RL یادگیری یک خط‌مشی بهینه است که پاداش‌های تجمعی را در طول زمان به حداکثر می‌رساند. عامل ازطریق آزمون‌وخطا محیط را بررسی می‌کند، اقداماتی را براساس وضعیت فعلی آن انجام می‌دهد و بازخورد دریافت می‌کند. از این بازخورد برای به‌روزرسانی خط‌مشی خود و اتخاذ تصمیم‌های بهتر در آینده استفاده می‌کند. الگوریتم‌های RL اغلب از توابع یا value functions برای تخمین پاداش‌ها یا مقادیر موردانتظار مرتبط با حالات و اقدامات مختلف استفاده می‌کنند که عامل را قادر می‌کند توانایی‌های تصمیم‌گیری خود را یاد بگیرد و بهبود بخشد. با هر تعامل و یادگیری مکرر، خط‌مشی عامل به‌تدریج به‌سمت یک راه‌حل بهینه همگرا می‌شود و به رفتار هوشمندانه و سازگار در محیط‌های پیچیده و پویا می‌انجامد"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512eeb8",
   "metadata": {
    "direction": "ltr"
   },
   "source": [
    "<div>\n",
    "            <img src=\"یادگیری-تقویتی.png\"/width='600'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c400e",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'> \n",
    "کاربردهای یادگیری تقویتی"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bc6c0",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "یادگیری تقویتی کاربردهای متعددی در حوزه‌های مختلف پیدا کرده است، از جمله:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22b6b6",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "+ رباتیک: RL ربات‌ها را قادر می‌کند تا عمل‌ها و حرکت‌های خود را براساس آزمون‌وخطا یاد بگیرند و بهبود بخشند و به آن‌ها اجازه می‌دهد در محیط‌های پیچیده حرکت کنند یا اشیا را دستکاری کنند.  \n",
    "      \n",
    "+ بازی: الگوریتم‌های RL در انجام‌دادن بازی‌های پیچیده، مانند شطرنج، Go و بازی‌های ویدئویی، به موفقیت چشمگیری دست یافته‌اند و در برخی موارد از عملکرد انسان پیشی گرفته‌اند  \n",
    "      \n",
    "+ وسایل نقلیه خودمختار: تکنیک‌های RL را می‌توان برای آموزش خودروهای خودران برای تصمیم‌گیری بهینه در زمان واقعی به کار برد که به حمل‌ونقل ایمن‌تر و کارآمدتر می‌انجامد  \n",
    "      \n",
    "+ مدیریت منابع: RL می‌تواند برای بهینه‌سازی تخصیص منابع، زمان‌بندی و تصمیم‌گیری در حوزه‌هایی مانند مدیریت انرژی، لجستیک و ارتباطات استفاده شود"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002682ca",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'> \n",
    "محدودیت‌های یادگیری تقویتی:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73945379",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>   \n",
    "درحالی‌که یادگیری تقویتی قابلیت‌های قدرتمندی را ارائه می‌کند، با محدودیت‌های خاصی نیز همراه است:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6ece0",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "+ کارایی نمونه (Sample Efficiency): الگوریتم‌های RL معمولاً به مقدار قابل توجهی از تعامل با محیط برای یادگیری سیاست‌های بهینه نیاز دارند و از نظر محاسباتی گران و وقت‌گیر هستند  \n",
    "      \n",
    "+ تریدآف اکتشاف و بهره‌برداری (Exploration-Exploitation Trade-off): ایجاد تعادل میان اکتشاف اقدامات جدید و بهره‌برداری از دانش آموخته‌شده چالش‌برانگیز است؛ زیرا اکتشاف بیش‌ازحد ممکن است تصمیم‌گیری بهینه را به تاخیر بیندازد و بهره‌برداری بیش‌ازحد ممکن است به راه‌حل‌های غیربهینه بینجامد  \n",
    "      \n",
    "+ مهندسی پاداش (Reward Engineering): طراحی توابع پاداش مناسب که با رفتار مدنظر هماهنگ باشد می‌تواند پیچیده باشد و تعریف پاداش‌هایی که به‌طور دقیق هدف‌های عامل را نشان می‌دهند، یک کار غیرضروری است  \n",
    "      \n",
    "+ ملاحظه‌های اخلاقی (Ethical Considerations): الگوریتم‌های یادگیری تقویتی می‌توانند رفتارهای نامطلوب یا مضر را بیاموزند، اگر به‌دقت طراحی نشده باشند، به‌طور بالقوه نگرانی‌های اخلاقی و نیاز به نظارت دقیق را افزایش می‌دهند.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d1c2b",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>   \n",
    "پارادایم یادگیری:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968777d",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "+ یادگیری نظارت‌شده: در یادگیری نظارت‌شده الگوریتم از نمونه‌های برچسب‌گذاری‌شده یاد می‌گیرد، جایی که هر ورودی با یک برچسب هدف یا خروجی مربوطه مرتبط است. هدف نگاشت ورودی‌ها به خروجی‌های ازپیش‌تعریف‌شده براساس داده‌های آموزشی ارائه شده است.  \n",
    "      \n",
    "+ یادگیری بدون نظارت: یادگیری بدون نظارت با داده‌های بدون برچسب سروکار دارد و بر کشف الگوها، ساختارها یا رابطه‌های پنهان در داده‌ها تمرکز دارد. این الگوریتم ویژگی‌های ذاتی داده‌ها را برای کشف اطلاعات معنادار بدون راهنمایی صریح بررسی می‌کند  \n",
    "      \n",
    "+ یادگیری تقویتی: RL در محیطی عمل می‌کند که عامل ازطریق آزمون‌وخطا یاد می‌گیرد. با محیط تعامل می‌کند، اقداماتی انجام می‌دهد، بازخوردی را به‌شکل پاداش یا جریمه دریافت می‌کند و رفتار خود را برای به‌حداکثررساندن پاداش‌های انباشته در طول زمان تنظیم می‌کند  \n",
    "      \n",
    "+ یادگیری نظارت‌شده: در یادگیری نظارت‌شده الگوریتم بازخورد مستقیم را در قالب داده‌های برچسب‌دار دریافت می‌کند. هدف آن به‌حداقل‌رساندن اختلاف میان برچسب‌های پیش‌بینی‌شده و واقعی، بهینه‌سازی عملکرد خطا یا ضرر ازپیش‌تعریف‌شده است.  \n",
    "      \n",
    "+ یادگیری بدون نظارت: یادگیری بدون نظارت بازخورد صریح یا برچسب‌های حقیقی ندارد. این الگوریتم ساختار ذاتی داده‌ها را با خوشه‌بندی، کاهش ابعاد یا مدل‌سازی مولد بررسی می‌کند.  \n",
    "      \n",
    "+ یادگیری تقویتی: در RL عامل از بازخورد تأخیری و پراکنده در قالب پاداش می‌آموزد. عامل یک سیگنال پاداش براساس اقدامات خود دریافت می‌کند و هدف آن یادگیری سیاستی است که پاداش تجمعی بلندمدت را به حداکثر می‌رساند\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d007a2",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>   \n",
    " پر استفاده‌ترین الگوریتم‌های یادگیری تقویتی کدام هستند؟     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63a39d",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>   \n",
    "      \n",
    "Q-learning و SARSA (سرنام State-Action-Reward-State-Action) دو الگوریتم محبوب و مستقل از مدل برای یادگیری تقویتی هستند. تمایز این الگوریتم‌ها با یکدیگر در استراتژی‌های جست‌و‌جوی آن‌ها محسوب می‌شود در حالیکه استراتژی‌های استخراج آن‌ها مشابه است. در حالیکه Q-learning یک روش مستقل از سیاست است که در آن عامل ارزش‌ها را براساس عمل a* که از سیاست دیگری مشتق شده می‌آموزد، SARSA یک روش مبتنی بر سیاست محسوب می‌شود که در آن ارزش‌ها را براساس عمل کنونی a که از سیاست کنونی آن مشتق شده می‌آموزد. پیاده‌سازی این دو روش آسان است اما فاقد تعمیم‌پذیری هستند زیرا دارای توانایی تخمین ارزش‌ها برای حالت‌های مشاهده نشده نیستند"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e07c3",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "با استفاده از الگوریتم‌های پیشرفته‌تری مانند Deep Q-Networks که از شبکه‌های عصبی برای تخمین Q-value‌ها استفاده می‌کنند می‌توان بر این چالش‌ها غلبه کرد. اما، DQN‌ها تنها می‌توانند فضای حالت گسسته و ابعاد کم را مدیریت کنند. DDPG (سرنام Deep Deterministic Policy Gradient) یک الگوریتم مستقل از مدل، مستقل از سیاست و عامل-نقاد (actor-critic) به شمار می‌آید که روش مواجهه آن با مساله، یادگیری سیاست‌هایی در فضای عمل ابعاد بالا و پیوسته است."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d15999",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>  \n",
    "      \n",
    "الگوریتم سارسا (SARSA):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5589d",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "الگوریتم سارسا (Sarsa) یک الگوریتم یادگیری تقویتی on-policy است. در این الگوریتم ابتدا عامل یادگیرنده با مشاهده حالت سیستم (S) و بر اساس سیاست مشخص، اقدام (A) را انتخاب می‌کند. در ادامه بعد از انتخاب اقدام، محیط حالت بعدی سیستم و پاداش را مشخص میکند. عامل با مشاهده وضعیت بعدی سیستم و پاداش دریافتی، مقدار تابع ارزش اقدام (action-value function) را محاسبه و به روز می‌کند. این روند تا زمانی که مقدار تابع ارزش اقدام به مقدار بهینه آن همگرا شود ادامه خواهد یافت.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec045cd",
   "metadata": {},
   "source": [
    "<div>\n",
    "            <img src=\"reml.png\"/width='800'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f845cf35",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'> \n",
    "جزئیات الگوریتم سارسا (Sarsa) در شکل زیر نشان داده شده است:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03468995",
   "metadata": {
    "direction": "ltr"
   },
   "source": [
    "<div>\n",
    "            <img src=\"sarsa.png\"/width='800'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8674ee",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "همانطور که نشان داده شده است، در مرحله اول می‌بایست مقادیر اولیه آلفا و اپسیلون و همچنین مقادیر مربوط به ماتریس Q(S,a) هم مشخص شود. سپس یک حالت به صورت تصادفی انتخاب می‌شود. با سیاست در نظر گرفته شده اقدام در این حالت اتخاذ می‌شود و پاداش (R) و مقدار بعدی حالت سیستم دریافت می‌شود. با استفاده از مقادیر مشاهده شده، مقدار Q(s,a) به روزرسانی می‌شود. همانطور که مشخص است به دلیل on-policy بودن این روش، مقدار ارزش اقدام به صورت مستقیم توسط اقدام و سیاست فعلی به روز می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a001810",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>  \n",
    "      \n",
    "الگوریتم Q-Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00862a67",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>   \n",
    "      \n",
    "الگوریتم یادگیری کیو (یادگیری Q) و یا Q-Learning، یکی از الگوریتم های بسیار معروف از نوع off-policy در حوزه یادگیری تقویتی است. عامل یادگیرنده با الگوریتم Q-Learning، مشابه الگوریتم سارسا، بعد از مشاهده اقدامی (A) را انتخاب می‌کند. سپس محیط به عامل، حالت بعدی سیستم و پاداش مربوطه ناشی از اقدام اتخاذ شده را بر می‌گرداند. عامل با مشاهده اطلاعات دریافتی از محیط، اقدام بعدی را انتخاب می‌کند و این فرآیند تا زمان رسیدن به سیاست بهینه ادامه پیدا می‌کند. در شکل زیر جزئیات الگوریتم Q-Learning نمایش داده شده است:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00c273",
   "metadata": {},
   "source": [
    "<div>\n",
    "            <img src=\"Q Learn.png\"/width='800'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67122d95",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "همانطور که نشان داده شده است، در مرحله اول می‌بایست مقادیر اولیه آلفا و اپسیلون و همچنین مقادیر مربوط به ماتریس Q(S,a) هم مشخص شود. سپس یک حالت به صورت تصادفی انتخاب می‌شود. با سیاست در نظر گرفته شده اقدام در این حالت اتخاذ می‌شود و پاداش (R) و مقدار بعدی حالت سیستم دریافت می‌شود. با استفاده از مقادیر مشاهده شده، مقدار Q(s,a) به روزرسانی می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349a4a3",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "الگوریتم Q-Learning بر خلاف الگوریتم سارسا، یک الگوریتم Off-policy است که این موضوع در فرمول ارائه شده برای به روزرسانی مقدار ارزش – اقدام Q(s,a) مشخص است. مقدار به روزرسانی بر اساس بیشترین مقدار ارزش-اقدام انجام میگیرد (Max Q(s’,a)) و نه بر اساس Q(s’,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4776c",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>   \n",
    "مقایسه عملکرد الگوریتم Sarsa و الگوریتم Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf2c4b",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'> \n",
    "همانطور که ذکر شد الگوریتم سارسا یک الگوریتم on-policy و الگوریتم Q-learning یک الگوریتم off-policy است. بنابراین در رسیدن به سیاست بهینه اختلاف اساسی وجود دارد. حال سوالی که وجود دارد این است که در چه مواقعی از الگوریتم SARSA یا الگوریتم Q-Learning استفاده کنیم؟"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10352d3",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>   \n",
    "در برخی از مقالات اشاره شده است که الگوریتم SARSA سرعت همگرایی بیشتری نسبت به الگوریتم Q-learning دارد. همچنین در الگوریتم سارسا پردازش کمتری نسبت به الگوریتم یادگیری Q احتیاج است. البته بیان شده است که در صورتی که نیاز است تا در زمان کم و با هزینه کمتری سیاست بهینه به دست یابد (مثلاً برنامه ریزی یک ربات در محیط واقعی)، بهتر است از الگوریتم SARSA استفاده شود. در غیر اینصورت و در صورتی که یک مدل شبیه سازی از سیستم وجود دارد و تعداد تکرار بالا هزینه ای را ایجاد نمی‌کند، الگوریتم Q-learning مناسب تر است"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56dcbf",
   "metadata": {
    "direction": "ltr"
   },
   "source": [
    "<div>\n",
    "            <img src=\"trade q s.png\"/width='800'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44239d9f",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'> \n",
    "همانطور که در نمودار نیز نشان داده شده است، سرعت همگرایی SARSA بهتر است. ولی در برخی از مقالات بیان شده است که Q-Learning تخمین بهتری از سیاست بهینه به دست می‌آورد"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b493f5",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>  \n",
    "      \n",
    "روش‌های تصمیم‌گیری در یادگیری تقویتی"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4c9b0",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "چندین روش تصمیم‌گیری در یادگیری تقویتی وجود دارد که تفاوت آن‌ها عمدتا به‌ دلیل استراتژی‌های مختلفی است که برای کشف محیط خود استفاده می‌کنند. در ادامه به برخی از پرکاربردترین روش‌های تصمیم‌گیری در یادگیری تقویتی اشاره خواهیم کرد."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c2c3f3",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:19px'>  \n",
    "      \n",
    "روش‌های مبتنی بر ارزش (Value-Based):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74babaa",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>   \n",
    "این روش‌ها بر میانگین ارزش‌های آموخته‌شده تمرکز می‌کنند. روش‌های مبتنی بر ارزش شامل جدولی از مقادیر هستند که عامل باید آن‌ها را فرا بگیرد. این مقادیر نشان‌دهنده پاداش مورد انتظار برای انجام اقدامی خاص در یک وضعیت به‌خصوص است. مقادیر در این روش پس از هر تعامل با محیط، بر اساس تفاوت بین پاداش مورد انتظار و پاداش واقعی به‌روز می‌شوند. این به‌روزرسانی به عامل اجازه می‌دهد تا از تعداد بیشتری از حالت‌ها و اقدامات درس بگیرد و نتایج بهتری را به موقعیت‌های جدید تعمیم دهد. Q-Learning و Deep Q-Learning دو نمونه از روش‌های مبتنی بر ارزش هستند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb77022",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "در شکل زیر عامل از حالت s با عمل a به وضعیت شکل دوم (b) رسیده است. سپس حالت‌های s و a یک جفت شده‌اند. پس از تعیین جفت s,a، عامل انتخاب ارزش تخمینی جفت حالت- عمل بعدی را توسط عمل r انجام می‌دهد و به همین ترتیب به بیشترین ارزش هر جفت می‌رسد"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030e429",
   "metadata": {
    "direction": "ltr"
   },
   "source": [
    "<div>\n",
    "            <img src=\"0-l1GMeRD0c_iKWWvd.png\"/width='600'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd08f8",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>   \n",
    "یادگیری تقویتی مبتنی بر ارزش مانند داشتن یک نقشه گنج است که در آن ارزش مکان‌های مختلف را با حفاری و یافتن پاداش می‌آموزید."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562698d0",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'> \n",
    "تصور کنید در حال کاوش در یک جزیره جدید به دنبال گنجینه‌های پنهان هستید. شما نقشه‌ای دارید که می‌گوید کدام مکان‌ها ممکن است گنج داشته باشند. هر بار که در یکی از آن مکان‌ها مشغول حفاری می‌شوید، مقداری طلا پیدا می‌کنید. با گذشت زمان، متوجه می‌شوید که نقاط خاصی روی نقشه شما ارزشمندتر هستند؛ زیرا طلای بیشتری دارند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398941c",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>\n",
    "یادگیری تقویتی مبتنی بر ارزش مشابه همین روش است. در این مورد شما کاوش‌گر یا عامل هستید، جزیره محیط است، نقشه درک عامل از اینکه کدام اقدامات یا تصمیمات ارزشمند هستند و طلا پاداش‌هایی است که عامل دریافت می‌کند. کاوش‌گر با انجام مکرر اقدامات، دریافت پاداش و به‌روزرسانی درک خود از بهترین اقدامات برای به حداکثر رساندن پاداش، می‌آموزد که کدام اقدامات یا تصمیمات ارزشمندتر هستند"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f01ee5",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>\n",
    "در یادگیری تقویتی مبتنی بر ارزش، عامل بهترین اقدامات را برای به حداکثر رساندن پاداش‌ها بر اساس تعامل خود با محیط می‌آموزد"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34278004",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:19px'>  \n",
    "      \n",
    "روش یادگیری مونت کارلو (Monte Carlo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce2895",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "این روش نوعی الگوریتم یادگیری تقویتی است که از تجربه یاد می‌گیرد. عامل شروع یادگیری را با پاداش‌هایی که برای اقدامات مختلف دریافت می‌کند، کاوش تصادفی محیط و جمع‌آوری داده‌ها انجام می‌دهد. سپس از این داده‌ها برای تخمین ارزش هر جفت حالت- عمل استفاده می‌شود. در نهایت عامل اقدامی که بالاترین ارزش تخمینی را در پی دارد، انتخاب می‌کند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d71dba",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "تصور کنید سعی دارید بهترین راه را برای انجام یک بازی ویدیویی جدید بیابید. اما قوانین و اینکه کدام اقدامات منجر به بالاترین امتیاز می‌شوند را نمی‌دانید"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfa115",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "مونت کارلو در یادگیری تقویتی مانند این است که پس از چندین بار انجام بازی از دوستان خود راهنمایی بخواهید. شما بازی می‌کنید، اقداماتی را انجام می‌دهید و نتایج اقدامات خود را می‌بینید. هر بار که بازی را تمام می‌کنید، از دوستانتان می‌پرسید که در مورد اقدامات و نتیجه بازی‌تان چه فکر می‌کنند. در طول زمان، با جمع‌آوری نظرات آن‌ها پس از بازی‌های زیاد، شروع به دریافت ایده‌های خوب می‌کنید که کدام اقدامات منجر به امتیازات بیشتر می‌شوند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387706b7",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "به عبارت فنی‌تر، مونت کارلو توسط یادگیری از تجربیات قبلی با شبیه‌سازی بسیاری از قسمت‌های تعامل با محیط، کسب حداکثر پاداش را یاد می‌گیرد. پس از هر اپیزود از بازخوردی که از آن تجربیات دریافت کرده‌اید، استفاده می‌کنید تا درک خود را از اینکه کدام اقدامات خوب هستند و کدام‌ها خوب نیستند، بدون نیاز به دانستن قوانین دقیق بازی از همان ابتدا، بهبود ببخشید."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca97c6",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:19px'>  \n",
    "      \n",
    "روش‌های تفاوت زمانی (Temporal Difference)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd1bb7",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "روش‌های تفاوت زمانی یا TD (Temporal Difference) نوعی الگوریتم یادگیری تقویتی هستند که از تفاوت بین پاداش مورد انتظار و پاداش واقعی یاد می‌گیرند. عامل شروع یادگیری را با تخمین ارزش هر جفت حالت- عمل انجام می‌دهد. سپس این تخمین‌ها را پس از هر تعامل با محیط به‌روز می‌کند. به‌روزرسانی بر اساس تفاوت بین پاداش مورد انتظار و پاداش واقعی انجام می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3feaf",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:17px'>  \n",
    "      \n",
    "به‌طور مثال تصور کنید در حال پختن کلوچه هستید؛ اما هنوز زمان دقیق پخت را نمی‌دانید. بعد از پخته شدن تعدادی کلوچه، شما طعم و بافت هر کدام را با آن‌هایی که در دفعات قبلی پخته شده بودند، مقایسه می‌کنید. سپس تخمین خود را با در نظر گرفتن تفاوت دسته فعلی با موارد قبلی به‌روز می‌کنید. در این مثال شما عامل هستید و نحوه پخت کلوچه‌ها اعمال. حال در یادگیری تقویتی، هوش مصنوعی عامل است. با گذشت زمان و تکرارهای بیشتر، تخمین عامل هوش مصنوعی از بهترین زمان پخت دقیق‌تر می‌شود که منجر به نتایج بهتر می‌شود."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad215bae",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafd348",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "<span style='font-family:XB Niloofar;font-size:20px'>  \n",
    "      \n",
    "تمرین) درباره سایر روش های تصمیم گیری در یادگیری نظارتی مانند گرادیان خط مشی و شکل دهی پاداش تحقیق کنید "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122712d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
